<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Seurat: From Moving Points to Depth</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="img/cats.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website">
    <meta property="og:url" content="seurat-cvpr.github.io.git">
    <meta property="og:title" content="Seurat: From Moving Points to Depth">
    <meta property="og:description" content="">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-L4QKE1L396"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-L4QKE1L396');
</script>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 20px auto 0 auto; display: inline-block">
            <img src="img/logo.png" alt="Logo" style="height: 60px; vertical-align: middle; margin-bottom: 10px">
            <h2 class="col-md-12 text-center" id="title" style="display: inline;">
                Seurat: From Moving Points to Depth<br>
                <text style="font-size: 0.8em;">
                        CVPR 2025 Highlight
                </text>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:10px auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <a style="text-decoration:none" href="https://seokju-cho.github.io/">
                    Seokju&nbsp;Cho<sup>1</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://gabriel-huang.github.io">
                    Jiahui&nbsp;Huang<sup>2</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://cvlab.kaist.ac.kr">
                    Seungryong&nbsp;Kim<sup>1</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="http://joonyoung-cv.github.io">
                    Joon-Young&nbsp;Lee<sup>2</sup>
                </a>
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <sup>1</sup> KAIST AI
                        </td>
                        <td>
                            <sup>2</sup> Adobe Research
                        </td>
                    </tr>
                </table>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2407.15420">
                            <img src="./img/paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/KU-CVLAB/LocoTrack" target="_blank">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code (TBD)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://huggingface.co/spaces/hamacojr/LocoTrack" target="_blank">
                            <image src="img/huggingface_logo-noborder.svg" height="60px">
                                <h4><strong>Demo (TBD)</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-center">
                    <img src="./img/teaser.png" width="100%">
                </div>
                <div class="text-justify">
                    <strong>TL;DR:</strong> Seurat predicts precise and smooth depth changes for dynamic objects by only looking at the 2D point trajectories.
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Accurate depth estimation from monocular videos remains challenging due to ambiguities inherent in single-view geometry, as crucial depth cues like stereopsis are absent. However, humans often perceive relative depth intuitively by observing variations in the size and spacing of objects as they move. Inspired by this, we propose a novel method that infers relative depth by examining the spatial relationships and temporal evolution of a set of tracked 2D trajectories. Specifically, we use off-the-shelf point tracking models to capture 2D trajectories. Then, our approach employs spatial and temporal transformers to process these trajectories and directly infer depth changes over time. Evaluated on the TAPVid-3D benchmark, our method demonstrates robust zero-shot performance, generalizing effectively from synthetic to real-world datasets. Results indicate that our approach achieves temporally smooth, high-accuracy depth predictions across diverse domains.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
TBD
                    </textarea>
                </div>
            </div>
        </div>

        <!-- References section -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>References</h3>
                <div class="text-justify">
                    <p><a name="ref1"></a>[1] Doersch et al., "Tapir: Tracking any point with per-frame initialization and temporal refinement", CVPR 2023.</p>
                    <p><a name="ref2"></a>[2] Karaev et al., "CoTracker: It is Better to Track Together", ECCV 2024.</p>
                </div>
                <br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                <!-- We would like to thank Lior Yariv and Kai Zhang for helping us evaluate their methods, and Ricardo Martin-Brualla for helpful comments on our text. DV is supported by the National Science Foundation under Cooperative Agreement PHY-2019786 (an NSF AI Institute, <a href="http://iaifi.org">http://iaifi.org</a>) -->
                    <!-- <br> -->
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                </p>
            </div>
        </div>
    </div>


</body></html>
